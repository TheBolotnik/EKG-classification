from __future__ import annotations
import argparse
import os
import sys
import time
from pathlib import Path
from typing import Iterable
import requests
import pandas as pd

from src.config import save_config, project_root
from src.dataio import load_metadata, ensure_files_exist
from src.features import scp_to_superclass, build_features
from src.model import balance_undersample, train_xgb, save_model

PHYSIONET_BASE = "https://physionet.org/files/ptb-xl/1.0.1"
CSV_FILES = ["ptbxl_database.csv", "scp_statements.csv"]


# --- –ù–û–í–´–ô –ë–õ–û–ö: –±—ã—Å—Ç—Ä—ã–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–≥—Ä—É–∑–∫–∏ ---
import math
import time
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from tqdm import tqdm

PHYSIONET_BASE = "https://physionet.org/files/ptb-xl/1.0.1"
CSV_FILES = ["ptbxl_database.csv", "scp_statements.csv"]

def _requests_session():
    """Session —Å –ø—É–ª–æ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π –∏ —Ä–µ—Ç—Ä–∞—è–º–∏."""
    sess = requests.Session()
    retries = Retry(
        total=5, connect=5, read=5, backoff_factor=0.5,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=frozenset(["GET", "HEAD"])
    )
    adapter = HTTPAdapter(pool_connections=100, pool_maxsize=100, max_retries=retries)
    sess.mount("http://", adapter)
    sess.mount("https://", adapter)
    return sess

def download_file(url: str, dest: Path, chunk: int = 1 << 19, timeout: int = 60):
    """
    –°–∫–∞—á–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º (–µ—Å–ª–∏ –µ—Å—Ç—å Content-Length).
    chunk=512KB (–∫—Ä—É–ø–Ω–µ–µ, —á–µ–º –æ–±—ã—á–Ω–æ) ‚Äî —É—Å–∫–æ—Ä—è–µ—Ç.
    """
    dest.parent.mkdir(parents=True, exist_ok=True)
    sess = _requests_session()
    with sess.get(url, stream=True, timeout=timeout) as r:
        r.raise_for_status()
        total = int(r.headers.get("Content-Length") or 0)
        got = 0
        t0 = time.time()
        if total > 0:
            with open(dest, "wb") as f, tqdm(
                total=total, unit="B", unit_scale=True, desc=dest.name, leave=False
            ) as pbar:
                for blob in r.iter_content(chunk_size=chunk):
                    if blob:
                        f.write(blob)
                        got += len(blob)
                        pbar.update(len(blob))
        else:
            with open(dest, "wb") as f:
                for blob in r.iter_content(chunk_size=chunk):
                    if blob:
                        f.write(blob)
                        got += len(blob)
    dt = time.time() - t0
    # –∫–æ—Ä–æ—Ç–∫–∞—è —Å—Ç—Ä–æ–∫–∞-–ª–æ–≥:
    print(f"‚úì {dest.name}: {got/1e6:.1f} MB –∑–∞ {dt:.1f}s")

def _download_one(url: str, dest: Path, retries: int = 3) -> tuple[str, bool, str]:
    """–†–∞–±–æ—á–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—É–ª–∞ –ø–æ—Ç–æ–∫–æ–≤: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–∏–º—è, ok, err)."""
    if dest.exists():
        return (dest.name, True, "exists")
    try:
        download_file(url, dest)
        return (dest.name, True, "")
    except Exception as e:
        err = f"{type(e).__name__}: {e}"
        return (dest.name, False, err)

def parallel_download(pairs: list[tuple[str, Path]], max_workers: int = 16):
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è —Å–∫–∞—á–∫–∞ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤.
    pairs: [(url, Path), ...]
    """
    if not pairs:
        return
    # —É–±–µ—Ä—ë–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
    pairs = [(u, p) for (u, p) in pairs if not p.exists()]
    if not pairs:
        print("‚úì –í—Å—ë —É–∂–µ —Å–∫–∞—á–∞–Ω–æ.")
        return

    print(f"‚Üì –°–∫–∞—á–∏–≤–∞—é {len(pairs)} —Ñ–∞–π–ª–æ–≤, –ø–æ—Ç–æ–∫–æ–≤: {max_workers}")
    ok_cnt, fail_cnt = 0, 0
    with ThreadPoolExecutor(max_workers=max_workers) as ex, tqdm(total=len(pairs), desc="FILES", unit="file") as pbar:
        futures = {ex.submit(_download_one, url, dest): (url, dest) for url, dest in pairs}
        for fut in as_completed(futures):
            name, ok, err = fut.result()
            if ok:
                ok_cnt += 1
            else:
                fail_cnt += 1
                # –º–æ–∂–Ω–æ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ:
                # print(f"‚úó {name} -> {err}")
            pbar.update(1)
    if fail_cnt:
        print(f"‚ö†Ô∏è –£–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å: {ok_cnt}, –æ—à–∏–±–æ–∫: {fail_cnt} (–ø–æ–≤—Ç–æ—Ä–∏ –∑–∞–ø—É—Å–∫ ‚Äî –µ—Å—Ç—å —Ä–µ—Ç—Ä–∞–∏).")
    else:
        print(f"‚úì –°–∫–∞—á–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ: {ok_cnt} —Ñ–∞–π–ª–æ–≤.")

def ensure_csvs(base_dir: Path):
    pairs = []
    for name in CSV_FILES:
        target = base_dir / name
        if not target.exists():
            url = f"{PHYSIONET_BASE}/{name}"
            pairs.append((url, target))
    parallel_download(pairs, max_workers=int(os.getenv("DL_WORKERS", "8")))
    for name in CSV_FILES:
        target = base_dir / name
        if target.exists():
            print(f"‚úì CSV: {target}")

def download_records_subset(base_dir: Path, filenames_lr: Iterable[str], limit: int):
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∫–∞—á–∞–µ–º –ø–∞—Ä—ã .hea/.dat –¥–ª—è –ø–µ—Ä–≤—ã—Ö N –ø—É—Ç–µ–π.
    """
    pairs = []
    n = 0
    for rel in filenames_lr:
        if n >= limit:
            break
        for ext in (".hea", ".dat"):
            url = f"{PHYSIONET_BASE}/{rel}{ext}"
            dest = base_dir / f"{rel}{ext}"
            if not dest.exists():
                pairs.append((url, dest))
        n += 1
    workers = int(os.getenv("DL_WORKERS", "16"))
    parallel_download(pairs, max_workers=workers)
    print(f"‚úì –ì–æ—Ç–æ–≤–æ: {n} –∑–∞–ø–∏—Å–µ–π (–ø–æ 2 —Ñ–∞–π–ª–∞ –Ω–∞ –∑–∞–ø–∏—Å—å).")

def ensure_records(base_dir: Path, mode: str, subset_size: int):
    """
    mode:
      - 'csvs'   : —Ç–æ–ª—å–∫–æ CSV (–±–µ–∑ —Å–∏–≥–Ω–∞–ª–æ–≤)
      - 'subset' : CSV + –ø–µ—Ä–≤—ã–µ subset_size –∑–∞–ø–∏—Å–µ–π WFDB (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)
      - 'full'   : CSV + –≤—Å–µ –∑–∞–ø–∏—Å–∏ WFDB (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, –î–û–õ–ì–û!)
    """
    ensure_csvs(base_dir)

    if mode == "csvs":
        print("–†–µ–∂–∏–º 'csvs': —Å–∏–≥–Ω–∞–ª—ã WFDB –Ω–µ —Å–∫–∞—á–∏–≤–∞–µ–º.")
        return

    db = pd.read_csv(base_dir / "ptbxl_database.csv")

    if mode == "subset":
        filenames = db["filename_lr"].tolist()
        print(f"‚Üì –ü–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ WFDB: {subset_size} –∑–∞–ø–∏—Å–µ–π")
        download_records_subset(base_dir, filenames, subset_size)
        return

    if mode == "full":
        filenames = db["filename_lr"].tolist()
        print(f"‚ö†Ô∏è –ü–æ–ª–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ WFDB: {len(filenames)} –∑–∞–ø–∏—Å–µ–π ‚Äî –¥–æ–ª–≥–æ –∏ –º–Ω–æ–≥–æ –º–µ—Å—Ç–∞.")
        download_records_subset(base_dir, filenames, limit=len(filenames))
        return


def main():
    ap = argparse.ArgumentParser(description="Setup PTB-XL locally (download) and train model.")
    ap.add_argument("--ptbxl-dir", default=str(Path(project_root()) / "data" / "ptb-xl" / "1.0.1"),
                    help="–ö—É–¥–∞ —Å–∫–∞—á–∏–≤–∞—Ç—å/–∏—Å–∫–∞—Ç—å PTB-XL (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ./data/ptb-xl/1.0.1)")
    ap.add_argument("--download", choices=["csvs", "subset", "full"], default="subset",
                    help="–ß—Ç–æ —Å–∫–∞—á–∏–≤–∞—Ç—å: —Ç–æ–ª—å–∫–æ CSV, –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ WFDB, –∏–ª–∏ –≤—Å—ë —Ü–µ–ª–∏–∫–æ–º")
    ap.add_argument("--subset-size", type=int, default=500, help="–†–∞–∑–º–µ—Ä –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ WFDB –≤ —Ä–µ–∂–∏–º–µ subset")
    ap.add_argument("--leads", type=int, default=None, help="–°–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ–¥–µ–Ω–∏–π –±—Ä–∞—Ç—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—Å–µ)")
    ap.add_argument("--out", default="models/xgb.pkl", help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å")
    args = ap.parse_args()

    base_dir = Path(args.ptbxl_dir)
    base_dir.mkdir(parents=True, exist_ok=True)

    # 1) –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    ensure_records(base_dir, mode=args.download, subset_size=args.subset_size)

    # 2) —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Ç—å –≤ –∫–æ–Ω—Ñ–∏–≥
    cfg = {"base_path": str(base_dir)}
    save_config(cfg)
    print(f"‚úì –ü—É—Ç—å –∫ PTB-XL —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ ekg_config.yaml: {cfg['base_path']}")

    # 3) –æ–±—É—á–µ–Ω–∏–µ
    print("[1/5] Load metadata...")
    db, scp = load_metadata(str(base_dir))

    print("[2/5] Map SCP codes -> diagnostic_superclass...")
    meta = scp_to_superclass(db, scp)

    print("[3/5] Ensure WFDB files exist...")
    meta = ensure_files_exist(meta, str(base_dir))
    print(f"  usable records: {len(meta)}")
    if len(meta) == 0:
        print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö WFDB –∑–∞–ø–∏—Å–µ–π. –í—ã–±–µ—Ä–∏ --download subset/full.")
        sys.exit(1)

    print("[4/5] Build features...")
    X, y = build_features(meta, str(base_dir), leads=args.leads)
    print(f"  X: {X.shape}, classes: {y.value_counts().to_dict()}")

    print("[5/5] Balance, train, evaluate...")
    Xb, yb = balance_undersample(X, y)
    result = train_xgb(Xb, yb)
    result["feature_names"] = list(Xb.columns)

    os.makedirs(os.path.dirname(args.out), exist_ok=True)
    save_model(result, args.out)
    print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {args.out}")
    print("–ì–æ—Ç–æ–≤–æ ‚Äî –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –∫–æ–º–º–∏—Ç–∏—Ç—å (–ª—É—á—à–µ —á–µ—Ä–µ–∑ Git LFS –∏–ª–∏ Releases).")


if __name__ == "__main__":
    main()
